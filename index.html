<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="VADAR">
    <meta name="keywords" content="VADAR, Omni3D-Bench, Program Synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Recognize Any Object: A Benchmark and Model for Single Instance Recognition</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/vader_heart.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.9.0.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>



<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Recognize Any Object: A Benchmark and Model for Single
                            Instance Recognition</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://damianomarsili.github.io/" target="_blank"
                                    rel="noopener noreferrer">Damiano Marsili</a>,</span>
                            <span class="author-block">
                                <a href="https://aditya-mehta1.github.io/" target="_blank"
                                    rel="noopener noreferrer">Aditya Mehta</a>,</span>
                            <span class="author-block">
                                <a href="https://gkioxari.github.io/" target="_blank" rel="noopener noreferrer">Georgia
                                    Gkioxari</a></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Caltech</span>
                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Benchmark Link. -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                                                alt="Hugging Face logo" style="height: 1em;">
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>
                                <!-- Dataset Viewer. -->
                                <span class="link-block">
                                    <a href="sid.html" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            ðŸ‘€
                                        </span>
                                        <span>Dataset Viewer</span>
                                    </a>
                                </span>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="content has-text-centered">
                    <p>
                        <b>tl;dr</b> We introduce <b>Single Instance Recognition</b>, a novel task aimed at identifying
                        instances of objects in a zero-shot, open-world setting, along with an accompanying benchmark:
                        the <b>Single Instance Dataset</b>, and a baseline: <b>iFormer</b>.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->
        </div>
    </section>

    <section class="teaser video">
        <div class="container is-max-desktop">

            <div class="teaser-body">
                <video id="teaser" autoplay muted loop playsinline height="80%">
                    <source src="https://github.com/glab-caltech/vadar/raw/refs/heads/main/static/videos/vadar.mp4"
                        type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Visual perception excels at category-level recognition (e.g., cats vs. dogs) but struggles
                            with identifying specific instances within a category, which requires capturing subtle
                            spatial and appearance cues. We introduce the task of <b>Single Instance
                                Recognition</b> and the <b>Single Instance Dataset</b>â€”featuring 2,022 instances from 36
                            object types in diverse contexts. Our dataset offers <b>4-20x</b> more instances across a
                            broader object range than existing instance-centric benchmarks. Unlike fine-grained
                            classification, our setting is zero-shot and open-world. Vision foundation models struggle
                            on this task, but we establish a strong baseline by refining visual representations to
                            better capture spatial details. Our analysis reveals limitations in current methods, guiding
                            future research.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- SID. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Single Instance Dataset</h2>

                    <div class="content has-text-justified">
                        <p>
                            <b>Single Instance Dataset (SID)</b> comprises 2,022 instances from 36 categories of
                            common objects with over 47,000 images. Each instance is defined by positive images of
                            varying context and background, and is augmented with negatives. SID contains <b>4-20x</b>
                            more instances, several orders of magnitudes more images, and covers more categories than
                            other instance-specific datasets.
                        </p>
                        <img src="static/images/dataset-figure.jpeg" alt="Single Instance Dataset" width="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Approach. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Distribution of Instances</h2>

                    <div class="content has-text-justified">
                        <p>
                            We show the distribution of instances across the 36 categories and 8 supercategories in SID
                            below.
                        </p>
                        <img src="static/images/class_distribution.jpeg" alt="Single Instance Dataset" width="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- (b) Distribution ofinstances across the 36 categories and 8 supercategories in \ourdata. -->

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Approach. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Approach - iFormer</h2>

                    <div class="content has-text-justified">
                        <p>
                            iFormer projects embeddings from a vision model to generate query embeddings, predicting
                            instances via cosine similarity with reference images. A contrastive objective aligns
                            same-instance projections while separating negatives. For inference, instances
                            are predicted by taking the highest cosine similarity over a set of reference images and
                            comparing against a threshold.
                        </p>
                        <img src="static/images/method.jpeg" alt="Approach" width="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop is-fluid">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>
                    <div class="tabs is-boxed is-centered is-medium" id="tabs">
                        <ul>
                            <li data-tab="attn" class="is-active"><a>Attention Maps</a></li>
                            <li data-tab="qual"><a>Qualitative Examples</a></li>
                            <li data-tab="umap"><a>UMAP Plots</a></li>
                        </ul>
                    </div>
                    <div class="columns is-centered has-text-left">
                        <div class="column is-two-thirds content">
                            <p>All results shown are on <em>unseen</em> test instances.</p>
                        </div>
                    </div>
                    <!-- Attn Maps -->
                    <div id="tabs-content">
                        <div data-content="attn" class="tab-content is-active content has-text-left">
                            <div class="columns is-centered has-text-left">
                                <div class="column is-two-thirds content">
                                    We visualize attention maps for both iFormer and Native CLIP. iFormer focuses on key
                                    object features, such as the handle of the knife, the frame of the sunglasses, and
                                    the band of the watch. In contrast, CLIP - trained with text supervision and a
                                    different objective - produces a broader and less targeted attention distribution.
                                </div>
                            </div>
                            <img src="static/images/attn_maps.jpeg" alt="Approach" width="100%">

                        </div>
                        <!-- Qualitative Examples -->
                        <div data-content="qual" class="tab-content content">
                            <div class="columns is-centered has-text-left">
                                <div class="column is-two-thirds content">
                                    We show qualitative examples for vision foundation models and iFormer. For each
                                    query image, we show the image with the highest cosine similarity from a reference
                                    set. iFormer is more adept at matching colors and fine-grained details, especially
                                    with a CLIP backbone.
                                </div>
                            </div>
                            <img src="static/images/qualitative.jpeg" alt="Approach" width="100%">
                        </div>
                        <!-- UMAP Plots -->
                        <div data-content="umap" class="tab-content content">
                            <div class="columns is-centered has-text-left">
                                <div class="column is-two-thirds content">
                                    We show UMAP projections of iFormer embeddings across backbones.
                                    iFormer with a CLIP backbone shows strong instance-level clustering. With DINOv2 and
                                    SigLIP backbones, where iFormer performs worse, the clustering is less salient.
                                </div>
                            </div>
                            <img src="static/images/umap.jpeg" alt="Approach" width="100%">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
<code>TODO.</code></pre>
        </div>
    </section>

    <div id="modal" class="modal">
        <div class="modal-background"></div>

        <div class="modal-content">
            <div id='plot-loading-div' class="box">
                Loading...
            </div>
            <div id='plot-div'></div>
        </div>

        <button class="modal-close is-large" aria-label="close"></button>
    </div>

</body>

</html>